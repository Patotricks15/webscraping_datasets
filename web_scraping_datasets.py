# -*- coding: utf-8 -*-
"""Web scraping datasets

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JankVurInDUdLriwJwUlce3lTB2JOs1T

# WEB SCRAPING EM CONJUNTOS DE DADOS

O objetivo desse projeto é extrair nome e link de datasets de 5 sites:

1. Awesome Public Datasets
2. UCI Machine Learning
3. Google data search
4. Dados.gov
5. IBGE

Com esse notebook, espero contribuir com a comunidade de Ciência de Dados, facilitando a busca por datasets em sites internacionais e nacionais.

Você pode dar uma estrela no projeto através do Github, mas só se quiser.
"""

import requests
from bs4 import BeautifulSoup

def awesome_data(dados):
  nome_selec = []
  link_selec = []

  link = 'https://github.com/awesomedata/awesome-public-datasets'
  req = requests.get(link)
  soup = BeautifulSoup(req.content, 'html.parser')
  html = soup.find_all('a', rel="nofollow")
  print('\n\n-------------------- DATASETS ENCONTRADOS EM AWESOME PUBLIC DATASETS\n\n')

  for i in range(len(html))[8:]:
    nome = (str(html[i]).strip('</a>').split('w">')[1]).lower()
    link = str(html[i]).split('f="')[1].split('" r')[0]
    if (dados in nome):
      nome_selec.append(nome)
      link_selec.append(link)
    else:
      nome_selec += ''
      link_selec += ''
  for j in range(len(nome_selec)):
    if len(nome_selec) < 1:
      print('Não encontramos datasets')
    else:
      print(f'\n{nome_selec[j]}\n{link_selec[j]}\n')



def uci_dados(dados):
  dataset = dados
  dataset_limpo = dataset.replace(' ', '+').lower()

  link = f'https://archive.ics.uci.edu/ml/datasets/{dataset_limpo}'
  req = requests.get(link)
  soup = BeautifulSoup(req.content, 'html.parser')

  dataset_html = soup.find_all('span', class_='heading')
  tabela = soup.find_all('td')
  aviso = soup.find_all('p')
  print('\n\n-------------------- DATASETS ENCONTRADOS EM UCI\n\n')
  try:
    nome = str(dataset_html).split('b>')[1].split('</')[0]
    tipo = str(tabela).split('"normal">')[46].split('</p>')[0]
  except IndexError:
    nome = ''
    tipo = ''
    link = ''
    print('')

  print(f'\n{nome} ({link})\n{tipo}')

def data_google(dados):
  dataset = dados
  dataset_limpo = dataset.replace(' ', '+').lower()
  link = f'https://datasetsearch.research.google.com/search?query={dataset_limpo}'

  req = requests.get(link)
  soup = BeautifulSoup(req.content, 'html.parser')

  nome = soup.find_all('h1',  class_="iKH1Bc")
  site = soup.find_all('li', class_="iW1HZe")
  print('\n\n--------------------DATASETS ENCONTRADOS EM GOOGLE DATA SEARCH\n\n')
  print(f'Link para acessar esses e mais outros dados:{link}')
  for i in range(5):
    nome_limpo = str(nome[i]).split('">')[1].split('</')[0]
    site_limpo = str(site[i]).split('">')[1].split('<')[0]
    print(f'\n{nome_limpo}\n Origem do dataset: {site_limpo}\n')

def dados_gov(dados):
  dataset = dados
  dataset_limpo = dataset.replace(' ', '+').lower()
  link = f'https://dados.gov.br/dataset?q={dataset_limpo}'

  req = requests.get(link)
  soup = BeautifulSoup(req.content, 'html.parser')
  print('\n\n-------------------- DATASETS ENCONTRADOS EM DADOS.GOV\n\n')

  dataset_html = soup.find_all('h3', class_="dataset-heading")
  for i in range(len(dataset_html)):
    nome = str(dataset_html[i]).split('">')[2][:-len('</a>\n</h3>')]
    link_acesso = 'https://dados.gov.br' + str(dataset_html[i]).split('">')[1].split('="')[1]
    print(f'\n{nome} ({link_acesso})')

def ibge(dados):
  nome_selec = []
  link_selec = []
  dataset = dados
  dataset_limpo = dataset.replace(' ', '+').lower()
  link = f'https://www.ibge.gov.br/busca.html?searchword={dataset_limpo}'

  req = requests.get(link)
  soup = BeautifulSoup(req.content, 'html.parser')
  print('\n\n-------------------- DATASETS ENCONTRADOS EM IBGE\n\n')

  for i in range(5,15):
    html = soup.find_all('a',target="_blank")
    nome = str(html[i]).split('k">')[1].strip('</a>')
    link = str(html[i]).split('="')[1].split('" t')[0]
    if (dados in nome.lower()):
      nome_selec.append(nome)
      link_selec.append(link)
    else:
      nome_selec += ''
      link_selec += ''
  for j in range(len(nome_selec)):
    if len(nome_selec) < 1:
      print('Não encontramos datasets')
    else: 
      print(f'\n{nome_selec[j]}\n{link_selec[j]}\n')

def procurar():
  dataset = input('Pesquisar dataset sobre: ')
  awesome_data(dataset)
  uci_dados(dataset)
  data_google(dataset)
  dados_gov(dataset)
  ibge(dataset)

procurar()